{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AllChildren-WordEmbedding.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jdck-ke9mYNW",
        "rsK76J5O1Fqr"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPA0z7h+MRA5zPX371EdyF7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xiaomeng-ma/ParentBERTo/blob/master/AllChildren_WordEmbedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i95Mjto7AR49",
        "outputId": "6f419e1e-a5c4-43d4-c045-6dbbffe125c0"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus.reader import CHILDESCorpusReader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.probability import FreqDist\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX7bDgCvRb3L"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip list | grep -E 'transformers|tokenizers'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdck-ke9mYNW"
      },
      "source": [
        "##Getting All Parents' Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5smpXRVAywW"
      },
      "source": [
        "corpus = \"/content/drive/My Drive/corpus data/Data/US-Data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80R6EkZlKlFh"
      },
      "source": [
        "Data = CHILDESCorpusReader(corpus, '.*.xml')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqcHkRZERKqw"
      },
      "source": [
        "all_data = []\n",
        "for f in Data.fileids():\n",
        "  sents = Data.sents(f, speaker = ['MOT', 'FAT'])\n",
        "  word2sent = []\n",
        "  for sent in sents:\n",
        "    strsent = \" \".join(i for i in sent)\n",
        "    word2sent.append(strsent)\n",
        "  sents_in_file = \"\\n\".join(word2sent)\n",
        "  all_data.append([f, sents_in_file])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6WkjNxOhr1g"
      },
      "source": [
        "df=pd.DataFrame(all_data, columns=['file','parent sentence'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd74vwnuh6Ws"
      },
      "source": [
        "df.to_csv('All Parents Sentence')\n",
        "!cp 'All Parents Sentence' \"/content/drive/My Drive/Dissertation\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsK76J5O1Fqr"
      },
      "source": [
        "##Train a word embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tf0hyVgpcNeB"
      },
      "source": [
        "url = \"https://raw.githubusercontent.com/xiaomeng-ma/CHILDES/master/All%20Parents%20Sentence.csv\"\n",
        "df = pd.read_csv(url)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgF3rYcbe4Ae"
      },
      "source": [
        "df= df.dropna()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYAdUydHSPlT",
        "outputId": "8980ea5a-7e3a-4a63-a969-891a184d6080"
      },
      "source": [
        "len(df['parent sentence'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5150"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CELEnW17VOd",
        "outputId": "aa4ced74-4dae-4938-805d-c7828bf79131"
      },
      "source": [
        "df['parent sentence']"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       what's that\\nit's a chicken\\nyeah\\nwhat's this...\n",
              "1       wanna stack them for me\\n\\n\\n\\noh\\ndid you say...\n",
              "2       try another one\\nthose are very hard\\nlet me c...\n",
              "3       you like to open the doors of the house\\nthat'...\n",
              "4       dog\\nuhoh\\ndog\\n dog\\ntwo dogs\\ntwo more ladie...\n",
              "                              ...                        \n",
              "6902    what are you doing\\nyour gonna find the eggs\\n...\n",
              "6903    I thought it was a spider\\nit looks like a spi...\n",
              "6904    it looks like uh Harry Potter or something\\nis...\n",
              "6905    where\\nyeah what what is this\\nno what\\nis it ...\n",
              "6906    he doesn't watch the Smurfs or anything\\nI jus...\n",
              "Name: parent sentence, Length: 5150, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys0g1llG7dXB"
      },
      "source": [
        "import tensorflow as tf\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "    num_words=None,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t',\n",
        "    lower=True, split='\\n', char_level=False, oov_token=None)\n",
        "toked = tokenizer.fit_on_texts(df['parent sentence'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ha8pb4Ud-nKQ",
        "outputId": "30c6514e-f52e-47d0-f924-47c01c81be07"
      },
      "source": [
        "len(tokenizer.word_index) + 1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "666383"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc4ynUnnQ3IZ"
      },
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "trainer = BpeTrainer(vocab_size = 52_000, min_frequency = 2, special_tokens = [  \n",
        "    \"<pad>\",\n",
        "    \"<s>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",])\n",
        "tokenizer.train_from_iterator(df['parent sentence'],vocab_size = 52_000, min_frequency = 2, special_tokens = [  \n",
        "    \"<s>\",\n",
        "    \"<pad>\",\n",
        "    \"</s>\",\n",
        "    \"<unk>\",\n",
        "    \"<mask>\",])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17oZerZDTBB4",
        "outputId": "d9dbfe19-5296-4f98-aa59-fe73baa1a7f5"
      },
      "source": [
        "#!mkdir \"/content/drive/My Drive/Dissertation/ParentBERTo\"\n",
        "dir = \"/content/drive/My Drive/Dissertation/ParentBERTo\"\n",
        "tokenizer.save_model(dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/Dissertation/ParentBERTo/vocab.json',\n",
              " '/content/drive/My Drive/Dissertation/ParentBERTo/merges.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQhEKAc1U62w"
      },
      "source": [
        "from tokenizers.implementations import ByteLevelBPETokenizer\n",
        "from tokenizers.processors import BertProcessing\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    '/content/drive/My Drive/Dissertation/ParentBERTo/vocab.json',\n",
        " '/content/drive/My Drive/Dissertation/ParentBERTo/merges.txt',\n",
        ")\n",
        "tokenizer._tokenizer.post_processor= BertProcessing(\n",
        "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
        "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
        ")\n",
        "tokenizer.enable_padding(direction = \"right\", pad_id = 1, pad_type_id = 0, pad_token = \"<pad>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SDd2uoobYLb",
        "outputId": "e65ef465-fa33-4d4b-92ce-91aad0e2b257"
      },
      "source": [
        "encode = tokenizer.encode(\"I think he's coming\")\n",
        "print(encode.tokens)\n",
        "print(encode.ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s>', 'I', 'Ġthink', 'Ġhe', \"'s\", 'Ġcoming', '</s>']\n",
            "[0, 45, 399, 301, 279, 1315, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Xyw1zpMbzRU",
        "outputId": "2539b480-ecff-43bc-dced-79ceb42f23b2"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CE21EM6fXKs"
      },
      "source": [
        "from transformers import RobertaConfig\n",
        "\n",
        "config = RobertaConfig(\n",
        "    vocab_size=52_000,\n",
        "    max_position_embeddings=128,\n",
        "    num_attention_heads=8,\n",
        "    num_hidden_layers=4,\n",
        "    type_vocab_size=1,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtOgmCn_fh07"
      },
      "source": [
        "from transformers import RobertaTokenizerFast\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained(dir, max_len=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_MtO9LGfl_o"
      },
      "source": [
        "from transformers import RobertaForMaskedLM\n",
        "\n",
        "model = RobertaForMaskedLM(config=config)\n",
        "print(\"Number of parameters:\", model.num_parameters())\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlKfKFPOjcau"
      },
      "source": [
        "with open(\"/content/drive/My Drive/Dissertation/Parent-Sents.txt\", \"w\") as f:\n",
        "  for text in df['parent sentence'].tolist():\n",
        "    f.write(text + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8skUYIuXhjm0",
        "outputId": "43dfa9ac-5da9-4aad-c376-8f4f8979b3cb"
      },
      "source": [
        "from transformers import LineByLineTextDataset\n",
        "dataset = LineByLineTextDataset(\n",
        "tokenizer = tokenizer,\n",
        "file_path = \"/content/drive/My Drive/Dissertation/Parent-Sents.txt\",\n",
        "block_size = 128,\n",
        "    \n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:128: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFZbo_Bomtgq"
      },
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_pWw8oDnUzd"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=dir,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,\n",
        "    per_gpu_train_batch_size=32,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t3Hc4lYfniUv",
        "outputId": "3138ccf5-131e-4749-9511-aac4ce4c26e8"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='36478' max='36478' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [36478/36478 1:07:18, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>7.066000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>6.007100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>5.597000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>5.261100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>5.041900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>4.936900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>4.804900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>4.720600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>4.602800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>4.518100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>4.403100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>4.411400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>4.360200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>4.250300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>4.271600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>4.172200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>4.230100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>4.117800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>4.112200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>4.093000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>4.060300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>4.005500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>3.988600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>3.957500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>3.978300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>3.966500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>3.932900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>3.915700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>3.946500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>3.869600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>3.877200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>3.887000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>3.820000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>3.806400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>3.847400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>3.770200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>3.755500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>3.747200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>3.741600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>3.725500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>3.750300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>3.720100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>3.709300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>3.737900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>3.651200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>3.699500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>3.694600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>3.662400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>3.605200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>3.616800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>3.630700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>3.606400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>3.648900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>3.652500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>3.634800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>3.586600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>3.606000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>3.533400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>3.580800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>3.550300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>3.560200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>3.554200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>3.540300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>3.522800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>3.559600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>3.525600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>3.527800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>3.594200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>3.507800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>3.588400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>3.518700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>3.540000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=36478, training_loss=3.9929319321373926, metrics={'train_runtime': 4039.2625, 'train_samples_per_second': 9.031, 'total_flos': 9504757263348096.0, 'epoch': 1.0, 'init_mem_cpu_alloc_delta': 327290, 'init_mem_gpu_alloc_delta': 276130304, 'init_mem_cpu_peaked_delta': 18306, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 773927, 'train_mem_gpu_alloc_delta': 835963904, 'train_mem_cpu_peaked_delta': 202714820, 'train_mem_gpu_peaked_delta': 3461976064})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZJgv7j-4Ylc"
      },
      "source": [
        "trainer.save_model(dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fszCXbekQwwT"
      },
      "source": [
        "##Language Model Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDalhjsY4cDN",
        "outputId": "4491a5d7-4f91-4904-91ae-8ebea90ce134"
      },
      "source": [
        "from transformers import pipeline\n",
        "fill_mask = pipeline(\n",
        "    'fill-mask',\n",
        "    model = dir,\n",
        "    tokenizer = dir\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at /content/drive/My Drive/Dissertation/ParentBERTo and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LGCfokI4lvX",
        "outputId": "25bb2e5e-9b7a-4090-8a7a-adac8ea063f5"
      },
      "source": [
        "fill_mask(\"I <mask> you\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.4584316611289978,\n",
              "  'sequence': 'I see you',\n",
              "  'token': 384,\n",
              "  'token_str': ' see'},\n",
              " {'score': 0.049191635102033615,\n",
              "  'sequence': 'I love you',\n",
              "  'token': 1041,\n",
              "  'token_str': ' love'},\n",
              " {'score': 0.03835704177618027,\n",
              "  'sequence': 'I got you',\n",
              "  'token': 449,\n",
              "  'token_str': ' got'},\n",
              " {'score': 0.03673850744962692,\n",
              "  'sequence': 'I know you',\n",
              "  'token': 378,\n",
              "  'token_str': ' know'},\n",
              " {'score': 0.029599659144878387,\n",
              "  'sequence': 'I told you',\n",
              "  'token': 1382,\n",
              "  'token_str': ' told'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5XwH3J-4rr5",
        "outputId": "7d356efd-eccf-43be-f8eb-2627b07ab1c6"
      },
      "source": [
        "fill_mask(\"I pick <mask> up\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.3976026773452759,\n",
              "  'sequence': 'I pick it up',\n",
              "  'token': 290,\n",
              "  'token_str': ' it'},\n",
              " {'score': 0.11565425246953964,\n",
              "  'sequence': 'I pick you up',\n",
              "  'token': 273,\n",
              "  'token_str': ' you'},\n",
              " {'score': 0.08241920918226242,\n",
              "  'sequence': 'I pick them up',\n",
              "  'token': 495,\n",
              "  'token_str': ' them'},\n",
              " {'score': 0.08196157962083817,\n",
              "  'sequence': 'I pick  up',\n",
              "  'token': 225,\n",
              "  'token_str': ' '},\n",
              " {'score': 0.039304427802562714,\n",
              "  'sequence': 'I pick him up',\n",
              "  'token': 502,\n",
              "  'token_str': ' him'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbcymNP9QMRb",
        "outputId": "ca0d8f62-e070-4553-8daa-a0a7465b1a8a"
      },
      "source": [
        "fill_mask(\"I <mask> to shopping\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.21202704310417175,\n",
              "  'sequence': 'I got to shopping',\n",
              "  'token': 449,\n",
              "  'token_str': ' got'},\n",
              " {'score': 0.12016503512859344,\n",
              "  'sequence': 'I like to shopping',\n",
              "  'token': 349,\n",
              "  'token_str': ' like'},\n",
              " {'score': 0.0990045964717865,\n",
              "  'sequence': 'I want to shopping',\n",
              "  'token': 427,\n",
              "  'token_str': ' want'},\n",
              " {'score': 0.053962916135787964,\n",
              "  'sequence': 'I wanted to shopping',\n",
              "  'token': 1370,\n",
              "  'token_str': ' wanted'},\n",
              " {'score': 0.04086555913090706,\n",
              "  'sequence': 'I went to shopping',\n",
              "  'token': 775,\n",
              "  'token_str': ' went'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTPqxGfHQ3hN",
        "outputId": "5ec23378-2293-4be0-ea86-53984d781aa0"
      },
      "source": [
        "fill_mask(\"Paris is the <mask> of France\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.04591355472803116,\n",
              "  'sequence': 'Paris is the end of France',\n",
              "  'token': 1197,\n",
              "  'token_str': ' end'},\n",
              " {'score': 0.03489715978503227,\n",
              "  'sequence': 'Paris is the kind of France',\n",
              "  'token': 549,\n",
              "  'token_str': ' kind'},\n",
              " {'score': 0.030713440850377083,\n",
              "  'sequence': 'Paris is the picture of France',\n",
              "  'token': 822,\n",
              "  'token_str': ' picture'},\n",
              " {'score': 0.028830023482441902,\n",
              "  'sequence': 'Paris is the rest of France',\n",
              "  'token': 1634,\n",
              "  'token_str': ' rest'},\n",
              " {'score': 0.02075226418673992,\n",
              "  'sequence': 'Paris is the name of France',\n",
              "  'token': 766,\n",
              "  'token_str': ' name'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUaV84cWMijc"
      },
      "source": [
        "## another childes model by smeylan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFD6SWWSMZwi"
      },
      "source": [
        "from transformers import pipeline, AutoModelForMaskedLM, AutoTokenizer\n",
        "model = AutoModelForMaskedLM.from_pretrained('smeylan/childes-bert')\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"smeylan/childes-bert\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OTYsxAQQbv2"
      },
      "source": [
        "Smey_fill_mask = pipeline(\n",
        "    'fill-mask',\n",
        "    model = model,\n",
        "    tokenizer = tokenizer\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF67EPaXOC4A",
        "outputId": "212a394e-5de6-4ce7-a4f6-681a661f0f4c"
      },
      "source": [
        "Smey_fill_mask(\"I [MASK] you\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.07764840126037598,\n",
              "  'sequence': 'i did you',\n",
              "  'token': 2106,\n",
              "  'token_str': 'did'},\n",
              " {'score': 0.059528592973947525,\n",
              "  'sequence': 'i am you',\n",
              "  'token': 2572,\n",
              "  'token_str': 'am'},\n",
              " {'score': 0.05386057496070862,\n",
              "  'sequence': 'i know you',\n",
              "  'token': 2113,\n",
              "  'token_str': 'know'},\n",
              " {'score': 0.04900241270661354,\n",
              "  'sequence': 'i love you',\n",
              "  'token': 2293,\n",
              "  'token_str': 'love'},\n",
              " {'score': 0.03938209265470505,\n",
              "  'sequence': 'i see you',\n",
              "  'token': 2156,\n",
              "  'token_str': 'see'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igck-8xAQAa5",
        "outputId": "8f063f43-1540-4d00-b841-b6b1c5358762"
      },
      "source": [
        "Smey_fill_mask(\"I pick [MASK] up\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.5061348676681519,\n",
              "  'sequence': 'i pick it up',\n",
              "  'token': 2009,\n",
              "  'token_str': 'it'},\n",
              " {'score': 0.16822730004787445,\n",
              "  'sequence': 'i pick them up',\n",
              "  'token': 2068,\n",
              "  'token_str': 'them'},\n",
              " {'score': 0.0479658804833889,\n",
              "  'sequence': 'i pick up up',\n",
              "  'token': 2039,\n",
              "  'token_str': 'up'},\n",
              " {'score': 0.04171311855316162,\n",
              "  'sequence': 'i pick you up',\n",
              "  'token': 2017,\n",
              "  'token_str': 'you'},\n",
              " {'score': 0.035892508924007416,\n",
              "  'sequence': 'i pick one up',\n",
              "  'token': 2028,\n",
              "  'token_str': 'one'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yk09My3GQGjV",
        "outputId": "88f4de22-79c5-44c4-8ecd-4be6eeb33101"
      },
      "source": [
        "Smey_fill_mask(\"I [MASK] to shopping\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.20618851482868195,\n",
              "  'sequence': 'i went to shopping',\n",
              "  'token': 2253,\n",
              "  'token_str': 'went'},\n",
              " {'score': 0.1555020958185196,\n",
              "  'sequence': 'i go to shopping',\n",
              "  'token': 2175,\n",
              "  'token_str': 'go'},\n",
              " {'score': 0.11096654832363129,\n",
              "  'sequence': 'i want to shopping',\n",
              "  'token': 2215,\n",
              "  'token_str': 'want'},\n",
              " {'score': 0.07313667982816696,\n",
              "  'sequence': 'i going to shopping',\n",
              "  'token': 2183,\n",
              "  'token_str': 'going'},\n",
              " {'score': 0.04240037873387337,\n",
              "  'sequence': 'i got to shopping',\n",
              "  'token': 2288,\n",
              "  'token_str': 'got'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkGv2-L-RB3N",
        "outputId": "31c49fae-1498-49fb-ef18-dfadc28ce406"
      },
      "source": [
        "Smey_fill_mask(\"Paris is the [MASK] of France\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'score': 0.9964656829833984,\n",
              "  'sequence': 'paris is the capital of france',\n",
              "  'token': 3007,\n",
              "  'token_str': 'capital'},\n",
              " {'score': 0.0015605653170496225,\n",
              "  'sequence': 'paris is the city of france',\n",
              "  'token': 2103,\n",
              "  'token_str': 'city'},\n",
              " {'score': 0.0008385310065932572,\n",
              "  'sequence': 'paris is the centre of france',\n",
              "  'token': 2803,\n",
              "  'token_str': 'centre'},\n",
              " {'score': 0.0002896166988648474,\n",
              "  'sequence': 'paris is the center of france',\n",
              "  'token': 2415,\n",
              "  'token_str': 'center'},\n",
              " {'score': 0.0001177552476292476,\n",
              "  'sequence': 'paris is the university of france',\n",
              "  'token': 2118,\n",
              "  'token_str': 'university'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    }
  ]
}